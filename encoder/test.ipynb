{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import roc_curve\n",
    "from scipy.optimize import brentq\n",
    "from scipy.interpolate import interp1d\n",
    "import audioread"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "speakers_per_batch, utterances_per_speaker = 5, 10\n",
    "embeds = torch.rand(size=(speakers_per_batch, utterances_per_speaker, 20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "similarity_weight = nn.Parameter(torch.tensor([10.]))\n",
    "similarity_bias = nn.Parameter(torch.tensor([-5.]))\n",
    "\n",
    "speakers_per_batch, utterances_per_speaker = embeds.shape[:2]\n",
    "\n",
    "# Inclusive centroids (1 per speaker). Cloning is needed for reverse differentiation\n",
    "# create inc centroid for each speaker in batch by finding mean of utterances AT EACH EMBEDDING IDX\n",
    "# then norm\n",
    "centroids_incl = torch.mean(embeds, dim=1, keepdim=True)\n",
    "centroids_incl = centroids_incl.clone() / (torch.norm(centroids_incl, dim=2, keepdim=True) + 1e-5)\n",
    "\n",
    "# Exclusive centroids (1 per utterance)\n",
    "# (mean of utterance at each embedding idx) - (each utterance)\n",
    "centroids_excl = (torch.sum(embeds, dim=1, keepdim=True) - embeds)\n",
    "centroids_excl /= (utterances_per_speaker - 1)\n",
    "centroids_excl = centroids_excl.clone() / (torch.norm(centroids_excl, dim=2, keepdim=True) + 1e-5)\n",
    "\n",
    "# Similarity matrix. The cosine similarity of already 2-normed vectors is simply the dot\n",
    "# product of these vectors (which is just an element-wise multiplication reduced by a sum).\n",
    "# We vectorize the computation for efficiency.\n",
    "sim_matrix = torch.zeros(speakers_per_batch, utterances_per_speaker,\n",
    "                            speakers_per_batch)\n",
    "mask_matrix = 1 - np.eye(speakers_per_batch, dtype=int)\n",
    "for j in range(speakers_per_batch):\n",
    "    # each row in mask_matrix represents 1 speaker in batch\n",
    "    mask = np.where(mask_matrix[j])[0] # indexes of 1s in mask_matrix\n",
    "    # compute cosine sim via dot product\n",
    "    sim_matrix[mask, :, j] = (embeds[mask] * centroids_incl[j]).sum(dim=2) # otherwise\n",
    "    sim_matrix[mask, :, j] *= similarity_weight\n",
    "    sim_matrix[j, :, j] = (embeds[j] * centroids_incl[j] + similarity_bias).sum(dim=1) # i = k\n",
    "    sim_matrix[j, :, j] *= similarity_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.2925)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ce = torch.nn.CrossEntropyLoss()\n",
    "x1 = torch.tensor([[-0.1, -0.2, -0.3, -0.4], [-0.1, -0.2, -0.3, -0.4]]).float()\n",
    "x2 = torch.tensor([0,1])\n",
    "ce(x1,x2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.log(torch.exp())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-981.6509,   17.1950,   17.3330,   17.0617,   17.2566],\n",
       "        [-978.8950,   21.1419,   20.7072,   21.3241,   19.3361],\n",
       "        [-979.3747,   20.3317,   20.5422,   20.6956,   20.0121],\n",
       "        [-977.5345,   20.8301,   21.5688,   21.6655,   22.4786],\n",
       "        [-975.2383,   23.2504,   24.1748,   24.3249,   23.6548],\n",
       "        [-979.3206,   19.6952,   18.3881,   20.2115,   19.3078],\n",
       "        [-976.4357,   23.6502,   21.1381,   22.7274,   22.1186],\n",
       "        [-972.7823,   25.9305,   25.3570,   26.2298,   25.9515],\n",
       "        [-974.3808,   25.1884,   25.2018,   24.9270,   25.1980],\n",
       "        [-978.8405,   19.0207,   20.3922,   20.9197,   19.4741]],\n",
       "       grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sim_matrix[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.8349, 1.7195, 1.7333, 1.7062, 1.7257],\n",
       "        [2.1105, 2.1142, 2.0707, 2.1324, 1.9336],\n",
       "        [2.0625, 2.0332, 2.0542, 2.0696, 2.0012],\n",
       "        [2.2465, 2.0830, 2.1569, 2.1665, 2.2479],\n",
       "        [2.4762, 2.3250, 2.4175, 2.4325, 2.3655],\n",
       "        [2.0679, 1.9695, 1.8388, 2.0211, 1.9308],\n",
       "        [2.3564, 2.3650, 2.1138, 2.2727, 2.2119],\n",
       "        [2.7218, 2.5930, 2.5357, 2.6230, 2.5952],\n",
       "        [2.5619, 2.5188, 2.5202, 2.4927, 2.5198],\n",
       "        [2.1160, 1.9021, 2.0392, 2.0920, 1.9474]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sim_matrix[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.7839, 1.7195, 1.7333, 1.7062, 1.7257],\n",
       "        [2.0339, 2.1142, 2.0707, 2.1324, 1.9336],\n",
       "        [2.0011, 2.0332, 2.0542, 2.0696, 2.0012],\n",
       "        [2.1588, 2.0830, 2.1569, 2.1665, 2.2479],\n",
       "        [2.4046, 2.3250, 2.4175, 2.4325, 2.3655],\n",
       "        [2.0107, 1.9695, 1.8388, 2.0211, 1.9308],\n",
       "        [2.2771, 2.3650, 2.1138, 2.2727, 2.2119],\n",
       "        [2.6675, 2.5930, 2.5357, 2.6230, 2.5952],\n",
       "        [2.4705, 2.5188, 2.5202, 2.4927, 2.5198],\n",
       "        [2.0493, 1.9021, 2.0392, 2.0920, 1.9474]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sim_matrix[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[2.7666, 2.4374, 1.6308, 2.0274, 1.9173, 2.1072, 1.9042, 1.7557, 2.0510,\n",
       "         2.2377],\n",
       "        [2.5159, 2.4360, 2.3751, 2.4724, 1.6673, 1.7300, 2.0701, 2.3316, 2.0284,\n",
       "         2.5851],\n",
       "        [2.2612, 2.1291, 3.0185, 2.3079, 2.6390, 2.1932, 1.8777, 2.1220, 2.2076,\n",
       "         1.8930],\n",
       "        [2.1561, 1.8996, 2.0605, 2.7901, 2.6472, 2.0333, 2.0961, 2.5265, 2.5153,\n",
       "         1.9770]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sim_matrix[[1, 2, 3, 4], :, 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GE2E Sim Mat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* why is joon's sim mat different to corj's?\n",
    "    * embeds is size (speakers/batch, utts/speaker, emb_size) (5, 10, 20) going in\n",
    "    * corj sim mat out (5, 10, 5) \n",
    "    * joon sim mat out (5, 5, 5)\n",
    "\n",
    "* how can I adjust joon's to match corj's?\n",
    "\n",
    "* use this to check their ge2e losses\n",
    "\n",
    "* use this to check their proto angular loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "def similarity_matrix(embeds):\n",
    "    \"\"\"\n",
    "    Computes the similarity matrix according the section 2.1 of GE2E.\n",
    "\n",
    "    :param embeds: the embeddings as a tensor of shape (speakers_per_batch, \n",
    "    utterances_per_speaker, embedding_size)\n",
    "    :return: the similarity matrix as a tensor of shape (speakers_per_batch,\n",
    "    utterances_per_speaker, speakers_per_batch)\n",
    "    \"\"\"\n",
    "    similarity_weight = nn.Parameter(torch.tensor([10.]))\n",
    "    similarity_bias = nn.Parameter(torch.tensor([-5.]))\n",
    "\n",
    "    speakers_per_batch, utterances_per_speaker = embeds.shape[:2]\n",
    "    \n",
    "    # Inclusive centroids (1 per speaker). Cloning is needed for reverse differentiation\n",
    "    # create inc centroid for each speaker in batch by finding mean of utterances AT EACH EMBEDDING IDX\n",
    "    # then norm\n",
    "    centroids_incl = torch.mean(embeds, dim=1, keepdim=True)\n",
    "    centroids_incl = centroids_incl.clone() / (torch.norm(centroids_incl, dim=2, keepdim=True) + 1e-5)\n",
    "\n",
    "    # Exclusive centroids (1 per utterance)\n",
    "    # (mean of utterance at each embedding idx) - (each utterance)\n",
    "    centroids_excl = (torch.sum(embeds, dim=1, keepdim=True) - embeds)\n",
    "    centroids_excl /= (utterances_per_speaker - 1)\n",
    "    centroids_excl = centroids_excl.clone() / (torch.norm(centroids_excl, dim=2, keepdim=True) + 1e-5)\n",
    "\n",
    "    # Similarity matrix. The cosine similarity of already 2-normed vectors is simply the dot\n",
    "    # product of these vectors (which is just an element-wise multiplication reduced by a sum).\n",
    "    # We vectorize the computation for efficiency.\n",
    "    sim_matrix = torch.zeros(speakers_per_batch, utterances_per_speaker,\n",
    "                                speakers_per_batch)\n",
    "    mask_matrix = 1 - np.eye(speakers_per_batch, dtype=int)\n",
    "    for j in range(speakers_per_batch):\n",
    "        # each row in mask_matrix represents 1 speaker in batch\n",
    "        mask = np.where(mask_matrix[j])[0] # indexes of 1s in mask_matrix\n",
    "        # compute cosine sim via dot product\n",
    "        sim_matrix[mask, :, j] = (embeds[mask] * centroids_incl[j]).sum(dim=2)\n",
    "        sim_matrix[j, :, j] = (embeds[j] * centroids_excl[j]).sum(dim=1)\n",
    "\n",
    "        #sim_matrix[:, :, j] = (embeds * centroids_incl[j]).sum(dim=2)\n",
    "\n",
    "       \n",
    "    sim_matrix = sim_matrix * similarity_weight + similarity_bias\n",
    "    return sim_matrix # (5, 1, 20) * (5, 10, 20) -> (5, 10, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 1, 20])"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "centroids_incl = torch.mean(embeds, dim=1, keepdim=True)\n",
    "centroids_incl = centroids_incl.clone() / (torch.norm(centroids_incl, dim=2, keepdim=True) + 1e-5)\n",
    "centroids_incl.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 10, 20])"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeds.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 1, 20])"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "similarity_matrix(embeds)[0] # corj sim mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[16.6742, 16.2385, 16.0487, 16.3170, 16.7079],\n",
       "        [21.3579, 20.6582, 21.1883, 20.3702, 20.4534],\n",
       "        [22.1896, 22.0102, 21.6861, 20.5429, 22.0982],\n",
       "        [18.3206, 17.0490, 16.0587, 16.7957, 16.6575],\n",
       "        [20.8070, 19.3250, 19.8346, 20.5987, 19.8000],\n",
       "        [18.2206, 17.1984, 17.2436, 17.2208, 16.6799],\n",
       "        [24.2771, 23.3234, 23.0234, 23.6963, 24.1507],\n",
       "        [21.3176, 19.9951, 21.1612, 20.0701, 20.4435],\n",
       "        [22.0853, 21.8682, 21.5481, 21.7785, 22.6327],\n",
       "        [16.3744, 16.5684, 16.7081, 17.0269, 16.4539]],\n",
       "       grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "similarity_matrix(embeds)[0] # corj without excl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GE2E Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cor_ge2e_loss(embeds):\n",
    "    \"\"\"\n",
    "    Computes the softmax loss according the section 2.1 of GE2E.\n",
    "    \n",
    "    :param embeds: the embeddings as a tensor of shape (speakers_per_batch, \n",
    "    utterances_per_speaker, embedding_size)\n",
    "    :return: the loss and the EER for this batch of embeddings.\n",
    "    \"\"\"\n",
    "    speakers_per_batch, utterances_per_speaker = embeds.shape[:2]\n",
    "    ce_loss = nn.CrossEntropyLoss()\n",
    "\n",
    "    # Loss\n",
    "    sim_matrix = similarity_matrix(embeds)\n",
    "    sim_matrix = sim_matrix.reshape((speakers_per_batch * utterances_per_speaker,  \n",
    "                                        speakers_per_batch))\n",
    "    temp = sim_matrix\n",
    "    ground_truth = np.repeat(np.arange(speakers_per_batch), utterances_per_speaker)\n",
    "    target = torch.from_numpy(ground_truth).long()\n",
    "    loss = ce_loss(sim_matrix, target)\n",
    "    \n",
    "    # EER (not backpropagated)\n",
    "    with torch.no_grad():\n",
    "        inv_argmax = lambda i: np.eye(1, speakers_per_batch, i, dtype=int)[0]\n",
    "        labels = np.array([inv_argmax(i) for i in ground_truth])\n",
    "        preds = sim_matrix.detach().cpu().numpy()\n",
    "\n",
    "        # Snippet from https://yangcha.github.io/EER-ROC/\n",
    "        fpr, tpr, thresholds = roc_curve(labels.flatten(), preds.flatten())           \n",
    "        eer = brentq(lambda x: 1. - x - interp1d(fpr, tpr)(x), 0., 1.)\n",
    "        \n",
    "    return loss, eer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def joon_ge2e_loss(x):\n",
    "    \n",
    "    speakers_per_batch, utterances_per_speaker = embeds.shape[:2]\n",
    "    w = nn.Parameter(torch.tensor(10.0))\n",
    "    b = nn.Parameter(torch.tensor(-5.0))\n",
    "    criterion  = nn.CrossEntropyLoss()\n",
    "\n",
    "    assert x.size()[1] >= 2\n",
    "\n",
    "    gsize = x.size()[1] # utterances per speaker\n",
    "    centroids = torch.mean(x, 1) # inc centroids without norm\n",
    "    stepsize = x.size()[0] # speakers per batch\n",
    "\n",
    "    cos_sim_matrix = []\n",
    "\n",
    "    for ii in range(0,gsize): \n",
    "        idx = [*range(0,gsize)]\n",
    "        idx.remove(ii)\n",
    "        exc_centroids = torch.mean(x[:,idx,:], 1)\n",
    "        cos_sim_diag    = F.cosine_similarity(x[:,ii,:],exc_centroids)\n",
    "        cos_sim         = F.cosine_similarity(x[:,ii,:].unsqueeze(-1),centroids.unsqueeze(-1).transpose(0,2))\n",
    "        cos_sim[range(0,stepsize),range(0,stepsize)] = cos_sim_diag\n",
    "        cos_sim_matrix.append(torch.clamp(cos_sim,1e-6))\n",
    "    temp = cos_sim_matrix\n",
    "    cos_sim_matrix = torch.stack(cos_sim_matrix,dim=1)\n",
    "    \n",
    "    torch.clamp(w, 1e-6)\n",
    "    cos_sim_matrix = torch.tensor(cos_sim_matrix)\n",
    "    cos_sim_matrix = cos_sim_matrix * w + b\n",
    "    \n",
    "\n",
    "    ground_truth = np.repeat(np.arange(speakers_per_batch), utterances_per_speaker)\n",
    "    target = torch.from_numpy(ground_truth).long()\n",
    "\n",
    "    label   = torch.from_numpy(np.asarray(range(0,stepsize)))\n",
    "    nloss = criterion(cos_sim_matrix.view(-1,stepsize), target)\n",
    "\n",
    "    # with torch.no_grad():\n",
    "    #     inv_argmax = lambda i: np.eye(1, speakers_per_batch, i, dtype=int)[0]\n",
    "    #     labels = np.array([inv_argmax(i) for i in ground_truth])\n",
    "    #     preds = sim_matrix.detach().cpu().numpy()\n",
    "\n",
    "    #     # Snippet from https://yangcha.github.io/EER-ROC/\n",
    "    #     fpr, tpr, thresholds = roc_curve(labels.flatten(), preds.flatten())           \n",
    "    #     eer = brentq(lambda x: 1. - x - interp1d(fpr, tpr)(x), 0., 1.)\n",
    "\n",
    "    return temp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Angular Proto Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# embed (5, 10, 20)\n",
    "# cent (5, 1, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ap_similarity_matrix(embeds):\n",
    "\n",
    "    w = nn.Parameter(torch.tensor(10.0))\n",
    "    b = nn.Parameter(torch.tensor(-5.0))\n",
    "    speakers_per_batch, utterances_per_speaker = embeds.shape[:2]\n",
    "\n",
    "    centroids = torch.mean(embeds[:,1:,:], dim=1, keepdim=True) # eq 6\n",
    "    centroids /= (utterances_per_speaker - 1) # eq 6\n",
    "    centroids = centroids.clone()/(torch.norm(centroids, dim=2, keepdim=True) + 1e-5) # normalise vector\n",
    "\n",
    "    query = embeds[:,0,:].unsqueeze(1) # should already be normalised from forward pass\n",
    "    #query = query.clone()/(torch.norm(query, dim=2, keepdim=True) + 1e-5) # normalise vector\n",
    "\n",
    "    # compute sed btwn every query and every speaker emb centroid\n",
    "    sim_matrix = torch.zeros(speakers_per_batch, 1, speakers_per_batch)\n",
    "    for i in range(speakers_per_batch):\n",
    "        sim_matrix[:, :, i] = (query * centroids[i]).sum(dim=2)\n",
    "    \n",
    "    sim_matrix = w*sim_matrix + b\n",
    "    return sim_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "def angular_proto_loss(embeds):\n",
    "\n",
    "    assert embeds.size()[1] >= 2\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    # # (5,1,20)\n",
    "    # out_anchor      = torch.mean(x[:,1:,:], dim=1, keepdim=True) # mean each utterance for each embed idx except first utt\n",
    "    # out_positive    = x[:,0,:] # first embed\n",
    "    # stepsize        = out_anchor.size()[0]\n",
    "\n",
    "    #############\n",
    "    # >> JOON <<\n",
    "    # cos_sim_matrix  = F.cosine_similarity(out_positive.unsqueeze(-1),out_anchor.unsqueeze(-1).transpose(0,2))\n",
    "    # torch.clamp(w, 1e-6)\n",
    "    # cos_sim_matrix = cos_sim_matrix * w + b\n",
    "    # label   = torch.from_numpy(np.asarray(range(0,stepsize))).long()\n",
    "    #############\n",
    "\n",
    "    ##############\n",
    "    # >> CORJ <<\n",
    "    ap_sm = ap_similarity_matrix(embeds)\n",
    "    ap_sm = ap_sm.reshape((speakers_per_batch, speakers_per_batch))\n",
    "    ground_truth = np.arange(speakers_per_batch)\n",
    "    target = torch.from_numpy(ground_truth).long()\n",
    "    ##############\n",
    "\n",
    "    loss = criterion(ap_sm, target)\n",
    "\n",
    "    # EER\n",
    "    with torch.no_grad():\n",
    "        inv_argmax = lambda i: np.eye(1, speakers_per_batch, i, dtype=int)[0]\n",
    "        labels = np.array([inv_argmax(i) for i in ground_truth])\n",
    "        preds = ap_sm.detach().cpu().numpy()\n",
    "\n",
    "        # Snippet from https://yangcha.github.io/EER-ROC/\n",
    "        fpr, tpr, thresholds = roc_curve(labels.flatten(), preds.flatten())           \n",
    "        eer = brentq(lambda x: 1. - x - interp1d(fpr, tpr)(x), 0., 1.)\n",
    "\n",
    "    return loss, eer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(1.6395, grad_fn=<NllLossBackward0>), 0.4500000000011706)"
      ]
     },
     "execution_count": 207,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "angular_proto_loss(embeds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(1.7346, grad_fn=<NllLossBackward0>), 0.49999999999997546)"
      ]
     },
     "execution_count": 211,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cor_ge2e_loss(embeds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 10, 5])"
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "similarity_matrix(embeds).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (5, 1, 20) * (5, 10, 20) -> (5, 10, 5)\n",
    "# (5, 1, 20) * (5,  1, 20) -> (5,  1, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[3.6840, 3.6257, 3.5540, 3.6594, 3.9523],\n",
       "        [3.3781, 3.5920, 3.2014, 3.6058, 3.6591],\n",
       "        [2.9513, 3.8061, 3.3136, 3.4410, 3.1781],\n",
       "        [3.6365, 3.7020, 3.6398, 3.7833, 4.0437],\n",
       "        [3.4237, 4.0259, 3.9089, 3.9259, 3.9273]], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "angular_proto_loss(embeds) # joon sim mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.2143, grad_fn=<NllLossBackward0>)"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "angular_proto_loss(embeds) # corj sim mat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Proto Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# embed (5, 10, 20)\n",
    "# cent (5, 1, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [],
   "source": [
    "def proto_loss(embeds, label=None):\n",
    "    \"\"\"Computes angular variant of prototypical loss.\n",
    "\n",
    "    Args:\n",
    "        embeds : \n",
    "        label (_type_, optional): _description_. Defaults to None.\n",
    "    \"\"\"\n",
    "    assert embeds.size()[1] >= 2\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    pwd = nn.PairwiseDistance(p=2)\n",
    "\n",
    "    out_anchor = torch.mean(embeds[:,1:,:], dim=1,) # eq 6 - find centroids\n",
    "    out_positive = embeds[:,0,:] # query - 0th utt embed array from each speaker\n",
    "    step = out_anchor.size()[0]\n",
    "\n",
    "    output  = -1 * (pwd(out_positive.unsqueeze(-1),out_anchor.unsqueeze(-1).transpose(0,2))**2) # eq 7 \n",
    "    label   = torch.from_numpy(np.asarray(range(0,step))).long()\n",
    "    nloss   = criterion(output, label)\n",
    "\n",
    "    return nloss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [],
   "source": [
    "def proto_sed(embeds):\n",
    "    # squared euclidian distance\n",
    "    speakers_per_batch, utterances_per_speaker = embeds.shape[:2]\n",
    "    pwd = nn.PairwiseDistance(p=2)\n",
    "\n",
    "    centroids = torch.mean(embeds[:,1:,:], dim=1, keepdim=True) # dont need to norm for sed\n",
    "    query = embeds[:,0,:].unsqueeze(1)\n",
    "\n",
    "    # compute sed btwn every query and every speaker emb centroid\n",
    "    out = torch.zeros(speakers_per_batch, 1, utterances_per_speaker)\n",
    "    for i in range(speakers_per_batch):\n",
    "        out[:, :, i] = -1* (pwd(centroids[i], query)**2)\n",
    "\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-1.6533, -1.6840, -1.5919, -1.6411, -1.2058,  0.0000,  0.0000,\n",
       "           0.0000,  0.0000,  0.0000]],\n",
       "\n",
       "        [[-1.9505, -1.6500, -1.6480, -1.6320, -1.3953,  0.0000,  0.0000,\n",
       "           0.0000,  0.0000,  0.0000]],\n",
       "\n",
       "        [[-2.4848, -1.4254, -1.7047, -1.8469, -1.9867,  0.0000,  0.0000,\n",
       "           0.0000,  0.0000,  0.0000]],\n",
       "\n",
       "        [[-2.1348, -2.0365, -2.2512, -1.9185, -1.5980,  0.0000,  0.0000,\n",
       "           0.0000,  0.0000,  0.0000]],\n",
       "\n",
       "        [[-2.2121, -1.3681, -1.5904, -1.5039, -1.4993,  0.0000,  0.0000,\n",
       "           0.0000,  0.0000,  0.0000]]])"
      ]
     },
     "execution_count": 231,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "proto_sed(embeds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(3.1325)"
      ]
     },
     "execution_count": 233,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "proto_loss(embeds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "4236b1bd8af5c88b200f8ae259f28c355fd90bec7f49eb3a4c44b52c2fc9f9b2"
  },
  "kernelspec": {
   "display_name": "Python 3.9.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
